{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network coursework\n",
    "\n",
    "from math import e, tanh, sqrt\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AREA   BFIHOST      FARL     FPEXT       LDP   PROPWET   RMED-1D  \\\n",
      "0    0.113654  0.436954  0.891444  0.301211  0.175194  0.328571  0.427018   \n",
      "1    0.158923  0.584238  0.835829  0.733132  0.276144  0.176190  0.318947   \n",
      "2    0.257193  0.380795  0.510695  0.203467  0.354229  0.341270  0.473333   \n",
      "3    0.425334  0.377616  0.878610  0.586957  0.471888  0.544444  0.443860   \n",
      "4    0.130097  0.273775  0.733155  0.560539  0.229784  0.265079  0.442456   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "518  0.102541  0.314040  0.900000  0.115850  0.115014  0.455556  0.453684   \n",
      "519  0.164335  0.498411  0.891444  0.288442  0.308622  0.277778  0.464912   \n",
      "520  0.128473  0.406225  0.887166  0.307375  0.210687  0.620635  0.619298   \n",
      "521  0.600158  0.398808  0.831551  0.358448  0.655320  0.569841  0.631930   \n",
      "522  0.182672  0.353245  0.463636  0.326307  0.376027  0.658730  0.667018   \n",
      "\n",
      "         SAAR  Index flood  \n",
      "0    0.249076     0.115641  \n",
      "1    0.115195     0.105811  \n",
      "2    0.350924     0.236827  \n",
      "3    0.343943     0.441694  \n",
      "4    0.207187     0.133236  \n",
      "..        ...          ...  \n",
      "518  0.342710     0.112310  \n",
      "519  0.234702     0.160926  \n",
      "520  0.481930     0.149102  \n",
      "521  0.500411     0.632580  \n",
      "522  0.591581     0.286263  \n",
      "\n",
      "[523 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "def import_and_pre_process(path='CWData.csv', ratio=0.2)->list[pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "    df=pd.read_csv(path)\n",
    "    print(df)\n",
    "    X=df.loc[:, df.columns != 'Index flood']\n",
    "    y=df['Index flood']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return(X_train.values, X_test.values, y_train.values, y_test.values)\n",
    "\n",
    "data=import_and_pre_process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14716325 0.29797724 0.0897595  0.91082626 0.00604704 0.48386022\n",
      "  0.12324746 0.30767909]\n",
      " [0.92605956 0.16169455 0.96583232 0.20579438 0.74156611 0.54667726\n",
      "  0.88447873 0.55399326]\n",
      " [0.53455922 0.195458   0.16622936 0.91648915 0.3278382  0.37208225\n",
      "  0.75982366 0.33882348]\n",
      " [0.95206245 0.73792777 0.59742924 0.88089516 0.04960591 0.25706269\n",
      "  0.46503633 0.22697854]\n",
      " [0.74390519 0.91920803 0.42377012 0.3421326  0.96155073 0.44485243\n",
      "  0.4890484  0.35164987]\n",
      " [0.90954847 0.56895548 0.76217459 0.81180672 0.34184904 0.43369433\n",
      "  0.84847177 0.36325535]\n",
      " [0.79393513 0.94099232 0.62321037 0.74162729 0.80509731 0.84503965\n",
      "  0.39505334 0.05126346]\n",
      " [0.40187627 0.67375623 0.45035039 0.74896315 0.35698243 0.63474414\n",
      "  0.1864198  0.74936944]]\n",
      "(8, 418)\n",
      "(8, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 8 is different from 418)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 99\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     98\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m---> 99\u001b[0m \u001b[43mproper_back_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_biases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_biases_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhidden_weight_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[115], line 69\u001b[0m, in \u001b[0;36mproper_back_prop\u001b[1;34m(X_train, y_train, hidden_biases, hidden_biases_grad, output_bias, hidden_weight, hidden_weight_grad, output_weight, epochs, lp)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden_diff\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden_weight\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 69\u001b[0m delta_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_diff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#* delta_output\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(delta_hidden\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden_weight\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 8 is different from 418)"
     ]
    }
   ],
   "source": [
    "def abs_error(y_target,y_pred)->float:\n",
    "    error_arr=y_target-y_pred\n",
    "\n",
    "    return error_arr\n",
    "\n",
    "#learning parameter\n",
    "lp = 0.1\n",
    "\n",
    "\n",
    "activation=\"tan\"\n",
    "\n",
    "epochs=2000\n",
    "\n",
    "momentum= 0\n",
    "\n",
    "hidden_biases = np.random.rand(1,8)\n",
    "hidden_biases_grad= np.zeros((8,8))\n",
    "output_bias = random.randint(1,100)\n",
    "\n",
    "\n",
    "# doing random \n",
    "hidden_weight= np.random.rand(8,8)\n",
    "hidden_weight_grad=np.zeros((8,8))\n",
    "output_weight = random.randint(1,100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def proper_back_prop(X_train, y_train, hidden_biases, hidden_biases_grad,output_bias, hidden_weight,hidden_weight_grad, output_weight,epochs,lp=0.1):\n",
    "\n",
    "    def activation(sig):\n",
    "        #print(sig)\n",
    "        value = (1) / (1 + (e ** (sig*-1)))\n",
    "        #print(value)\n",
    "        return value\n",
    "\n",
    "    def diff(node):\n",
    "        value = (node) * (1 - (node))\n",
    "        return value\n",
    "    \n",
    "    print(hidden_weight)\n",
    "\n",
    "    #input layer\n",
    "    sum = np.matmul(hidden_weight, X_train.T)\n",
    "    derivatve = activation(sum)\n",
    "    weight_sum= np.matmul(hidden_weight,derivatve)\n",
    "\n",
    "    #output layer\n",
    "    weight_sum+=output_bias\n",
    "\n",
    "    output=activation(weight_sum)\n",
    "\n",
    "    error= y_train-output\n",
    "\n",
    "    #backward pass\n",
    "\n",
    "    output_diff = diff(output)\n",
    "\n",
    "    delta_output = error * output_diff\n",
    "\n",
    "    hidden_diff = diff(derivatve)\n",
    "\n",
    "    print(hidden_diff.shape)\n",
    "\n",
    "    print(hidden_weight.shape)\n",
    "\n",
    "    delta_hidden = np.matmul(hidden_diff, hidden_weight.T) #* delta_output\n",
    "\n",
    "    print(delta_hidden.shape)\n",
    "\n",
    "    print(hidden_weight.shape)\n",
    "\n",
    "    hidden_weight += lp * delta_hidden.T\n",
    "\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        # forward pass on each hidden_weight\n",
    "        # forward pass\n",
    "\n",
    "        weight_sum= np.zeros((8,8))\n",
    "        weight_sum = hidden_weight * X_train.T + hidden_biases\n",
    "\n",
    " \n",
    "        sum_val_1= (w0 * val_1) + bias_1 + (w1to0 * val_2)\n",
    "        u_1 = activation(sum_val_1)\n",
    "        weight_sum+= (u_1 * w0o)\n",
    "    \n",
    "\n",
    "        pass\n",
    "    \"\"\"\n",
    "\n",
    "    # backward pass\n",
    "    pass\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = data\n",
    "proper_back_prop(X_train, y_train, hidden_biases, hidden_biases_grad,output_bias, hidden_weight,hidden_weight_grad, output_weight,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "\n",
    "path = 'CWData.csv'\n",
    "\n",
    "data = []\n",
    "\n",
    "training = []  # 60% of data\n",
    "validation = []  # 20% of data\n",
    "testing = []  # 20 % of data\n",
    "\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "# Skip the header row if it exists\n",
    "df = df.iloc[1:] if df.iloc[0].equals(df.columns) else df\n",
    "data = df.values.tolist()\n",
    "#print(data)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lp:  0.1\n",
      "momentum:  0\n",
      "weight:  -0.0001\n",
      "weight:  -0.0004\n",
      "weight:  0.0004\n",
      "weight:  -0.0006\n",
      "weight:  0.0004\n",
      "weight:  -0.0007\n",
      "weight:  0.0007\n",
      "weight:  -0.0007\n",
      "weight:  0.0002\n",
      "weight:  -0.0008\n",
      "weight:  -0.0002\n",
      "weight:  0.0002\n",
      "weight:  0.0003\n",
      "weight:  0.0002\n",
      "weight:  -0.0002\n",
      "weight:  0.0004\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "weight:  0\n",
      "bias:  0\n",
      "bias:  0\n",
      "bias:  0\n",
      "bias:  0\n",
      "bias:  0\n",
      "bias:  0\n",
      "bias:  0\n",
      "bias:  0\n",
      "bias:  0\n",
      "acc:  228.455300083594\n",
      "pred:  167.43033521181462\n",
      "msre:  0.20238317675806178\n",
      "rmse:  3989.736893666455\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n#LMS Algorithm done for comparison\\n\\nw8=1\\nfor i in range(len(testing)):\\n    for j in range(8):\\n\\n        bias = 1\\n        area = training[j][0]\\n        bfihost= training[j][1]\\n        farl=training[j][2]\\n        fpext= training[j][3]\\n        ldp= training[j][4]\\n        propwet= training[j][5]\\n        rmed=training[j][6]\\n        saar=training[j][7]\\n        flood=training[j][8]\\n\\n        error=flood-(w0+(w1*area)+(w2*bfihost) + (w3*farl)+(w4*fpext)\\n                              +(w5*ldp)+(w6*propwet) + (w7*rmed)+(w8*saar))\\n        w0=w0+lp*error*bias\\n        w1=w1+lp*error*area\\n        w2=w2+lp*error*bfihost\\n        w3 = w3 + lp * error * farl\\n        w4 = w4 + lp * error * fpext\\n\\n        w5 = w5 + lp * error * ldp\\n        w6 = w6 + lp * error * propwet\\n        w7 = w7 + lp * error * rmed\\n        w8 = w8 + lp * error * saar\\n\\n    #print(\"Loop: \" +str((i+1)))\\n    error_arr.append(error*-1)\\n    weight_set.append([w0, w1, w2, w3, w4, w5, w6, w7, w8])\\n    epoch_arr.append(i)\\n\\nprint(weight_set)\\n\\nplt.plot(epoch_arr, error_arr, \\'r+\\')\\nplt.ylabel(\\'errors\\')\\nplt.xlabel(\\'epochs\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting the data into training, validation and training sets\n",
    "length = len(data)\n",
    "\n",
    "train_val = round(0.6 * length)\n",
    "\n",
    "for i in range(0, train_val):\n",
    "    training.append(data[i])\n",
    "\n",
    "val_test = round(0.8 * length)\n",
    "\n",
    "for i in range(train_val, val_test):\n",
    "    validation.append(data[i])\n",
    "\n",
    "for i in range(val_test, length):\n",
    "    testing.append(data[i])\n",
    "\n",
    "\n",
    "# weights from inputs to hidden layer\n",
    "\n",
    "\n",
    "w0 = .1\n",
    "w1 = .2\n",
    "w2 = .3\n",
    "w3 = .1\n",
    "w4 = .2\n",
    "w5 = .3\n",
    "w6 = .1\n",
    "w7 = .2\n",
    "\n",
    "# weights from hidden layer to output\n",
    "w0o = .1\n",
    "w1o = .2\n",
    "w2o = .3\n",
    "w3o = .1\n",
    "w4o = .2\n",
    "w5o = .3\n",
    "w6o = .1\n",
    "w7o = .2\n",
    "\n",
    "\n",
    "#learning parameter\n",
    "lp = 0.1\n",
    "\n",
    "bias_1 = 1\n",
    "bias_2 = 1\n",
    "bias_3 = 1\n",
    "bias_4 = 1\n",
    "bias_5 = 1\n",
    "bias_6 = 1\n",
    "bias_7 = 1\n",
    "bias_8 = 1\n",
    "bias_9 = 1\n",
    "\n",
    "activation=\"tan\"\n",
    "\n",
    "epochs=2000\n",
    "\n",
    "w0to1= 1\n",
    "w1to2 = 2\n",
    "w2to3 = 3\n",
    "w3to4 = 1\n",
    "w4to5 = 2\n",
    "w5to6  = 3\n",
    "w6to7 = 1\n",
    "\n",
    "w1to0 = 1\n",
    "w2to1 = 2\n",
    "w3to2 = 3\n",
    "w4to3 = 1\n",
    "w5to4 = 2\n",
    "w6to5 = 3\n",
    "w7to6 = 1\n",
    "\n",
    "test_set=training\n",
    "\n",
    "momentum= 0\n",
    "\n",
    "actual_arr=[]\n",
    "pred_arr=[]\n",
    "error_arr = []\n",
    "epoch_arr = []\n",
    "actual=[]\n",
    "pred=[]\n",
    "\n",
    "counter=1000\n",
    "\n",
    "\n",
    "# backpropagation algorithm\n",
    "weight_set = np.array([w0,w0o,w1,w1o,w2,w2o,w3,w3o,w4,w4o,w5,w5o,w6,w6o,w7,w7o])\n",
    "bias_set = np.array([bias_1, bias_2, bias_3, bias_4, bias_5, bias_6, bias_7, bias_8, bias_9])\n",
    "\n",
    "extra_weights = np.array([w1to0,w2to1,w3to2,w4to3 ,w5to4,w6to5,w7to6, w0to1, w1to2, w2to3, w3to4, w4to5, w5to6, w6to7])\n",
    "\n",
    "def msre(pred_arr, actual_arr):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Relative Error between predicted and actual values.\n",
    "    \n",
    "    Args:\n",
    "        pred_arr (list): List of predicted values\n",
    "        actual_arr (list): List of actual values\n",
    "        \n",
    "    Returns:\n",
    "        float: Mean squared relative error\n",
    "    \"\"\"\n",
    "    value = 0\n",
    "    top = 0\n",
    "    for index in range(len(pred_arr)):\n",
    "        top = (pred_arr[index] - actual_arr[index])\n",
    "        bottom = actual_arr[index]\n",
    "        value += (top/bottom)**2\n",
    "\n",
    "    value *= (1/len(pred_arr))\n",
    "    return value\n",
    "\n",
    "\n",
    "def ce(pred, actual, obs_mean):\n",
    "    \"\"\"\n",
    "    Calculate Coefficient of Efficiency (Nash-Sutcliffe model efficiency coefficient).\n",
    "    \n",
    "    Args:\n",
    "        pred (list): List of predicted values\n",
    "        actual (list): List of actual values\n",
    "        obs_mean (float): Mean of observed values\n",
    "        \n",
    "    Returns:\n",
    "        float: Coefficient of efficiency\n",
    "    \"\"\"\n",
    "    top = 0\n",
    "    for i in range(len(pred)):\n",
    "        top += (pred[i] - actual[i])**2\n",
    "    bottom = 0\n",
    "    for real in actual:\n",
    "        bottom += (real - obs_mean)**2\n",
    "    value = 1 - (top/bottom)\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def rsqr(actual_values, mean_obsv, pred_values, mean_mod):\n",
    "    \"\"\"\n",
    "    Calculate R-squared value (coefficient of determination).\n",
    "    \n",
    "    Args:\n",
    "        actual_values (list): List of actual values\n",
    "        mean_obsv (float): Mean of observed values\n",
    "        pred_values (list): List of predicted values\n",
    "        mean_mod (float): Mean of predicted values\n",
    "        \n",
    "    Returns:\n",
    "        float: R-squared value\n",
    "    \"\"\"\n",
    "    top = 0\n",
    "    for i in range(len(actual_values)):\n",
    "        top += (actual_values[i]-mean_obsv) * (pred_values[i]-mean_mod)\n",
    "    bottom = 0\n",
    "    for i in range(len(actual_values)):\n",
    "        bottom += ((actual_values[i] - mean_obsv)**2) * ((pred_values[i] - mean_mod)**2)\n",
    "    value = (top/sqrt(bottom))**2\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "def anneal(p, q, r, x):\n",
    "    \"\"\"\n",
    "    Perform simulated annealing for parameter adjustment.\n",
    "    \n",
    "    Args:\n",
    "        p (float): End parameter\n",
    "        q (float): Starting parameter\n",
    "        r (int): Maximum epochs\n",
    "        x (int): Current epoch\n",
    "        \n",
    "    Returns:\n",
    "        float: Annealed parameter value\n",
    "    \"\"\"\n",
    "    if x < r:\n",
    "        value = p + (q-p)\n",
    "        value *= (1 - ((1)/(1+e**(10-(20*x)/(r)))))\n",
    "        return value\n",
    "\n",
    "\n",
    "def decay(error, lp, epoch, weights, extra_weights):\n",
    "    \"\"\"\n",
    "    Apply weight decay to regularize the neural network.\n",
    "    \n",
    "    Args:\n",
    "        error (float): Current error\n",
    "        lp (float): Learning parameter\n",
    "        epoch (int): Current epoch\n",
    "        weights (list): List of weights\n",
    "        extra_weights (list): List of additional weights\n",
    "        \n",
    "    Returns:\n",
    "        float: Updated error with weight decay applied\n",
    "    \"\"\"\n",
    "    weight = weights + extra_weights\n",
    "    omega = 0.5 * sum(np.square(weight))\n",
    "    upsilon = 1 / (lp * epoch)\n",
    "    upsilon = 0.1\n",
    "    decay = error + upsilon * omega\n",
    "    return decay\n",
    "\n",
    "\n",
    "def bold_driver(lp, old, new, lim, interval, epoch):\n",
    "    \"\"\"\n",
    "    Apply bold driver technique to adjust learning rate.\n",
    "    \n",
    "    Args:\n",
    "        lp (float): Current learning rate\n",
    "        old (float): Previous error\n",
    "        new (float): Current error\n",
    "        lim (float): Threshold for significant change\n",
    "        interval (int): Interval of epochs to change learning rate\n",
    "        epoch (int): Current epoch\n",
    "        \n",
    "    Returns:\n",
    "        float: Updated learning rate\n",
    "    \"\"\"\n",
    "    if epoch % interval == 0:\n",
    "        change = (old-new)\n",
    "\n",
    "        if abs(change) > lim:\n",
    "            if change > 0:\n",
    "                return lp * 1.05\n",
    "            elif change < 0:\n",
    "                return lp * 0.7\n",
    "            elif change == 0:\n",
    "                return lp\n",
    "        else:\n",
    "            return lp\n",
    "    else:\n",
    "        return lp\n",
    "\n",
    "def back_prop(epoch, weights, extra_weight, bias, lp, momentum, set, acc, values):\n",
    "    \"\"\"\n",
    "    Implement backpropagation algorithm for neural network training.\n",
    "    \n",
    "    Args:\n",
    "        epoch (int): Number of training epochs\n",
    "        weights (list): Initial weights\n",
    "        extra_weight (list): Additional weights\n",
    "        bias (list): Initial bias values\n",
    "        lp (float): Learning parameter\n",
    "        momentum (float): Momentum parameter\n",
    "        set (list): Training data set\n",
    "        acc (str): Activation function type ('tan', 'sigmoid', or 'linear')\n",
    "        values (bool): Whether to manually input initial values\n",
    "        \n",
    "    Returns:\n",
    "        None: Results are printed and plotted\n",
    "    \"\"\"\n",
    "    \n",
    "    rmse = 0\n",
    "    print(\"lp: \",lp)\n",
    "    print(\"momentum: \", momentum)\n",
    "\n",
    "    actual_arr = []\n",
    "    pred_arr = []\n",
    "    error_arr = []\n",
    "    epoch_arr = []\n",
    "\n",
    "    if acc==\"tan\":\n",
    "        def activation(tan):\n",
    "            return tanh(tan)\n",
    "\n",
    "        def diff(node):\n",
    "            value = 1 - node**2\n",
    "            return value\n",
    "\n",
    "\n",
    "    elif acc==\"sigmoid\":\n",
    "\n",
    "        def activation(sig):\n",
    "            #print(sig)\n",
    "            value = (1) / (1 + (e ** (sig*-1)))\n",
    "            #print(value)\n",
    "            return value\n",
    "\n",
    "        def diff(node):\n",
    "            value = (node) * (1 - (node))\n",
    "            return value\n",
    "\n",
    "    elif acc==\"linear\":\n",
    "\n",
    "        def activation(lin):\n",
    "            return lin\n",
    "\n",
    "        def diff(node):\n",
    "            return 1\n",
    "    if values:\n",
    "        for x in range(len(weights)):\n",
    "            weights[x]= float(input(\"enter initial weights\"))\n",
    "            print(\"weight: \",weights[x])\n",
    "\n",
    "        for y in range(len(extra_weight)):\n",
    "            extra_weight[y] = float(input(\"enter extra weights\"))\n",
    "            print(\"weight: \",extra_weight[y])\n",
    "\n",
    "        for z in range(len(bias)):\n",
    "            bias[z] = float(input(\"enter biases\"))\n",
    "            print(\"bias: \",bias[z])\n",
    "    # weights from inputs to hidden layer\n",
    "    else:\n",
    "        for x in range(len(weights)):\n",
    "            weights[x] = round(random.uniform(-2 / len(set), 2 / len(set)), 4)\n",
    "            print(\"weight: \", weights[x])\n",
    "\n",
    "        for y in range(len(extra_weight)):\n",
    "            extra_weight[y] = round(random.uniform(-2 / len(set), 2 / len(set)), 4)\n",
    "            print(\"weight: \", extra_weight[y])\n",
    "\n",
    "        for z in range(len(bias)):\n",
    "            bias[z] = round(random.uniform(-2 / len(set), 2 / len(set)), 4)\n",
    "            print(\"bias: \", bias[z])\n",
    "\n",
    "    # weights from inputs to hidden layer\n",
    "    w0 = weights[0]\n",
    "    w1 = weights[2]\n",
    "    w2 = weights[4]\n",
    "    w3 = weights[6]\n",
    "    w4 = weights[8]\n",
    "    w5 = weights[10]\n",
    "    w6 = weights[12]\n",
    "    w7 = weights[14]\n",
    "\n",
    "\n",
    "    # weights from hidden layer to output\n",
    "    w0o = weights[1]\n",
    "    w1o = weights[3]\n",
    "    w2o = weights[5]\n",
    "    w3o = weights[7]\n",
    "    w4o = weights[9]\n",
    "    w5o = weights[11]\n",
    "    w6o = weights[13]\n",
    "    w7o = weights[15]\n",
    "\n",
    "\n",
    "    # extra weights\n",
    "    w1to0 = extra_weight[0]\n",
    "    w2to1 = extra_weight[1]\n",
    "    w3to2 = extra_weight[2]\n",
    "    w4to3 = extra_weight[3]\n",
    "    w5to4 = extra_weight[4]\n",
    "    w6to5 = extra_weight[5]\n",
    "    w7to6 = extra_weight[6]\n",
    "\n",
    "    w0to1 = extra_weight[7]\n",
    "    w1to2 = extra_weight[8]\n",
    "    w2to3 = extra_weight[9]\n",
    "    w3to4 = extra_weight[10]\n",
    "    w4to5 = extra_weight[11]\n",
    "    w5to6 = extra_weight[12]\n",
    "    w6to7 = extra_weight[13]\n",
    "\n",
    "\n",
    "    bias_1 = bias[0]\n",
    "    bias_2 = bias[1]\n",
    "    bias_3 = bias[2]\n",
    "    bias_4 = bias[3]\n",
    "    bias_5 = bias[4]\n",
    "    bias_6 = bias[5]\n",
    "    bias_7 = bias[6]\n",
    "    bias_8 = bias[7]\n",
    "\n",
    "    bias_9 = bias[8]\n",
    "\n",
    "\n",
    "    for x in range(epoch):\n",
    "        error_sum=0\n",
    "\n",
    "        for i in range(len(set)):\n",
    "\n",
    "            weight_sum=0\n",
    "\n",
    "            val_1= set[i][0]\n",
    "            val_2 = set[i][1]\n",
    "            val_3 = set[i][2]\n",
    "            val_4 = set[i][3]\n",
    "            val_5 = set[i][4]\n",
    "            val_6 = set[i][5]\n",
    "            val_7 = set[i][6]\n",
    "            val_8 = set[i][7]\n",
    "            val_9 = set[i][8]\n",
    "\n",
    "            # forward pass\n",
    "            sum_val_1= (w0 * val_1) + bias_1 + (w1to0 * val_2)\n",
    "            u_1 = activation(sum_val_1)\n",
    "            weight_sum+= (u_1 * w0o)\n",
    "\n",
    "            sum_val_2 = (w1 * val_2) + bias_2 + (w2to1 * val_3) + (w0to1* val_1)\n",
    "            u_2 = activation(sum_val_2)\n",
    "            weight_sum += (u_2 * w1o)\n",
    "\n",
    "            sum_val_3 = (w2 * val_3) + bias_3 + (w3to2 * val_4) + (w1to2 * val_2)\n",
    "            u_3 = activation(sum_val_3)\n",
    "            weight_sum+=(u_3 * w2o)\n",
    "\n",
    "            sum_val_4 = (w3 * val_4) + bias_4 + (w4to3 * val_5) + (w2to3 * val_3)\n",
    "            u_4 = activation(sum_val_4)\n",
    "            weight_sum += (u_4 * w3o)\n",
    "\n",
    "            sum_val_5 = (w4 * val_5) + bias_5 + (w3to4 * val_6) + (w5to4 * val_4)\n",
    "            u_5 = activation(sum_val_5)\n",
    "            weight_sum+= (u_5 * w4o)\n",
    "\n",
    "            sum_val_6 = (w5 * val_6) + bias_6 + (w6to5 * val_7) + (w4to5 * val_5)\n",
    "            u_6 = activation(sum_val_6)\n",
    "            weight_sum += (u_6 * w5o)\n",
    "\n",
    "            sum_val_7 = (w6 * val_7) + bias_7 + (w7to6 * val_8) + (w5to6 * val_6)\n",
    "            u_7 = activation(sum_val_7)\n",
    "            weight_sum+= (u_7 * w6o)\n",
    "\n",
    "            sum_val_8 = (w7 * val_8) + bias_8 + (w6to7 * val_7)\n",
    "            u_8 = activation(sum_val_8)\n",
    "            weight_sum+= (u_8 * w7o)\n",
    "\n",
    "            weight_sum+=bias_9\n",
    "\n",
    "            u_output = activation(weight_sum)\n",
    "\n",
    "            actual_val_9 = val_9 * (469.699 - 0.406) + 0.406\n",
    "            pred_val_9= u_output * (469.699 - 0.406) + 0.406\n",
    "\n",
    "\n",
    "\n",
    "            error = (val_9-u_output)\n",
    "\n",
    "            acc_error=error * ((469.699 - 0.406) + 0.406)\n",
    "\n",
    "            #error = decay(error,lp,x+1,weights, extra_weights)\n",
    "            #print(error)\n",
    "\n",
    "            # backward pass\n",
    "            u_output_diff = diff(u_output)\n",
    "\n",
    "            delta_output = (error) * u_output_diff\n",
    "\n",
    "\n",
    "\n",
    "            u_1_diff = diff(u_1)\n",
    "            delta_u1 = w0o * u_1_diff * delta_output\n",
    "\n",
    "            u_2_diff = diff(u_2)\n",
    "            delta_u2 = w1o * u_2_diff * delta_output\n",
    "\n",
    "            u_3_diff = diff(u_3)\n",
    "            delta_u3 = w2o * u_3_diff * delta_output\n",
    "\n",
    "            u_4_diff = diff(u_4)\n",
    "            delta_u4 = w3o * u_4_diff * delta_output\n",
    "\n",
    "            u_5_diff = diff(u_5)\n",
    "            delta_u5 = w4o * u_5_diff * delta_output\n",
    "\n",
    "            u_6_diff = diff(u_6)\n",
    "            delta_u6 = w5o * u_6_diff * delta_output\n",
    "\n",
    "            u_7_diff = diff(u_7)\n",
    "            delta_u7 = w6o * u_7_diff * delta_output\n",
    "\n",
    "            u_8_diff = diff(u_8)\n",
    "            delta_u8 = w7o * u_8_diff * delta_output\n",
    "\n",
    "            # update biases and weights\n",
    "\n",
    "            bias_1 += lp * delta_u1 * 1\n",
    "            bias_2 += lp * delta_u2 * 1\n",
    "            bias_3 += lp * delta_u3 * 1\n",
    "            bias_4 += lp * delta_u4 * 1\n",
    "            bias_5 += lp * delta_u5 * 1\n",
    "            bias_6 += lp * delta_u6 * 1\n",
    "            bias_7 += lp * delta_u7 * 1\n",
    "            bias_8 += lp * delta_u8 * 1\n",
    "\n",
    "            bias_9 += lp * delta_output * 1\n",
    "\n",
    "\n",
    "\n",
    "            w0to1 += lp * delta_u2 * u_2 + (momentum * (w0to1 - (w0to1 + lp * delta_u2 * u_2)))\n",
    "\n",
    "            w1to2 += lp * delta_u3 * u_3 + (momentum * (w1to2 - (w1to2 + lp * delta_u3 * u_3)))\n",
    "\n",
    "            w2to3 += lp * delta_u4 * u_4 + (momentum * (w2to3 - (w2to3 + lp * delta_u4 * u_4)))\n",
    "\n",
    "            w3to4 += lp * delta_u5 * u_5 + (momentum * (w3to4 - (w3to4 + lp * delta_u5 * u_5)))\n",
    "\n",
    "            w4to5 += lp * delta_u6 * u_6 + (momentum * (w4to5 - (w4to5 + lp * delta_u6 * u_6)))\n",
    "\n",
    "            w5to6 += lp * delta_u7 * u_7 + (momentum * (w5to6 - (w5to6 + lp * delta_u7 * u_7)))\n",
    "\n",
    "            w6to7 += lp * delta_u8 * u_8 + (momentum * (w6to7 - (w6to7 + lp * delta_u8 * u_8)))\n",
    "\n",
    "            w1to0 += lp * delta_u1 * u_1 + (momentum * (w1to0 - (w1to0 + lp * delta_u1 * u_1)))\n",
    "\n",
    "            w2to1 += lp * delta_u2 * u_2 + (momentum * (w2to1 - (w2to1 + lp * delta_u2 * u_2)))\n",
    "\n",
    "            w3to2 += lp * delta_u3 * u_3 + (momentum * (w3to2 - (w3to2 + lp * delta_u3 * u_3)))\n",
    "\n",
    "            w4to3 += lp * delta_u4 * u_4 + (momentum * (w4to3 - (w4to3 + lp * delta_u4 * u_4)))\n",
    "\n",
    "            w5to4 += lp * delta_u5 * u_5 + (momentum * (w5to4 - (w5to4 + lp * delta_u5 * u_5)))\n",
    "\n",
    "            w6to5 += lp * delta_u6 * u_6 + (momentum * (w6to5 - (w6to5 + lp * delta_u6 * u_6)))\n",
    "\n",
    "            w7to6 += lp * delta_u7 * u_7 + (momentum * (w7to6 - (w7to6 + lp * delta_u7 * u_7)))\n",
    "\n",
    "\n",
    "            w0 += lp * delta_u1 * u_1 + (momentum * (w0 - (w0 + lp * delta_u1 * u_1)))\n",
    "\n",
    "            w1 += lp * delta_u2 * u_2 + (momentum * (w1 - (w1 + lp * delta_u2 * u_2)))\n",
    "\n",
    "            w2 += lp * delta_u3 * u_3 + (momentum * (w2 - (w2 + lp * delta_u3 * u_3)))\n",
    "\n",
    "            w3 += lp * delta_u4 * u_4 + (momentum * (w3 - (w3 + lp * delta_u4 * u_4)))\n",
    "\n",
    "            w4 += lp * delta_u5 * u_5 + (momentum * (w4 - (w4 + lp * delta_u5 * u_5)))\n",
    "\n",
    "            w5 += lp * delta_u6 * u_6 + (momentum * (w5 - (w5 + lp * delta_u6 * u_6)))\n",
    "\n",
    "            w6 += lp * delta_u7 * u_7 + (momentum * (w6 - (w6 + lp * delta_u7 * u_7)))\n",
    "\n",
    "            w7 += lp * delta_u8 * u_8 + (momentum * (w7 - (w7 + lp * delta_u8 * u_8)))\n",
    "\n",
    "            w0o += lp * delta_u1 * u_1 + (momentum * (w0o - (w0o + lp * delta_u1 * u_1)))\n",
    "\n",
    "            w1o += lp * delta_u2 * u_2 + (momentum * (w1o - (w1o + lp * delta_u2 * u_2)))\n",
    "\n",
    "            w2o += lp * delta_u3 * u_3 + (momentum * (w2o - (w2o + lp * delta_u3 * u_3)))\n",
    "\n",
    "            w3o += lp * delta_u4 * u_4 + (momentum * (w3o - (w3o + lp * delta_u4 * u_4)))\n",
    "\n",
    "            w4o += lp * delta_u5 * u_5 + (momentum * (w4o - (w4o + lp * delta_u5 * u_5)))\n",
    "\n",
    "            w5o += lp * delta_u6 * u_6 + (momentum * (w5o - (w5o + lp * delta_u6 * u_6)))\n",
    "\n",
    "            w6o += lp * delta_u7 * u_7 + (momentum * (w6o - (w6o + lp * delta_u7 * u_7)))\n",
    "\n",
    "            w7o += lp * delta_u8 * u_8 + (momentum * (w7o - (w7o + lp * delta_u8 * u_8)))\n",
    "\n",
    "            rmse+=(acc_error)**2\n",
    "            \n",
    "\n",
    "\n",
    "        actual_arr.append(actual_val_9)\n",
    "        pred_arr.append(pred_val_9)\n",
    "        error_arr.append(acc_error)\n",
    "        epoch_arr.append(x)\n",
    "        #lp = bold_driver(lp, error_arr[x - 1], error_arr[x], 1, 1000, x)\n",
    "\n",
    "\n",
    "    avg_pred = sum(pred_arr) / len(pred_arr)\n",
    "    avg_error = sum(error_arr) / len(error_arr)\n",
    "    avg_actual = sum(actual_arr) / len(actual_arr)\n",
    "    #rootsquare = rsqr(actual_arr, avg_actual, pred_arr, avg_pred)\n",
    "    meansre= msre(pred_arr, actual_arr)\n",
    "    print(\"acc: \", actual_val_9)\n",
    "    print(\"pred: \", pred_val_9)\n",
    "    #print(\"rsqr: \", rootsquare)\n",
    "    print(\"msre: \", meansre)\n",
    "\n",
    "    #cofe=ce(pred_arr, actual_arr, avg_actual)\n",
    "    rmse = sqrt(rmse / len(epoch_arr))\n",
    "    print(\"rmse: \",rmse)\n",
    "    #print(\"ce: \", cofe)\n",
    "\n",
    "    plt.plot(epoch_arr, error_arr, 'r+')\n",
    "    plt.ylabel('errors')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show()\n",
    "\n",
    "back_prop(epochs,weight_set, extra_weights, bias_set,lp,momentum,test_set, activation, False)\n",
    "\n",
    "\n",
    "'''\n",
    "#LMS Algorithm done for comparison\n",
    "\n",
    "w8=1\n",
    "for i in range(len(testing)):\n",
    "    for j in range(8):\n",
    "\n",
    "        bias = 1\n",
    "        area = training[j][0]\n",
    "        bfihost= training[j][1]\n",
    "        farl=training[j][2]\n",
    "        fpext= training[j][3]\n",
    "        ldp= training[j][4]\n",
    "        propwet= training[j][5]\n",
    "        rmed=training[j][6]\n",
    "        saar=training[j][7]\n",
    "        flood=training[j][8]\n",
    "\n",
    "        error=flood-(w0+(w1*area)+(w2*bfihost) + (w3*farl)+(w4*fpext)\n",
    "                              +(w5*ldp)+(w6*propwet) + (w7*rmed)+(w8*saar))\n",
    "        w0=w0+lp*error*bias\n",
    "        w1=w1+lp*error*area\n",
    "        w2=w2+lp*error*bfihost\n",
    "        w3 = w3 + lp * error * farl\n",
    "        w4 = w4 + lp * error * fpext\n",
    "\n",
    "        w5 = w5 + lp * error * ldp\n",
    "        w6 = w6 + lp * error * propwet\n",
    "        w7 = w7 + lp * error * rmed\n",
    "        w8 = w8 + lp * error * saar\n",
    "\n",
    "    #print(\"Loop: \" +str((i+1)))\n",
    "    error_arr.append(error*-1)\n",
    "    weight_set.append([w0, w1, w2, w3, w4, w5, w6, w7, w8])\n",
    "    epoch_arr.append(i)\n",
    "\n",
    "print(weight_set)\n",
    "\n",
    "plt.plot(epoch_arr, error_arr, 'r+')\n",
    "plt.ylabel('errors')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
